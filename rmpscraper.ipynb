{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1pTVLpdD9hAgg9ofX_bgWDnBs_JaJOt5-",
      "authorship_tag": "ABX9TyNFP2dK1lCPH0KmlWcdcRd2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ritvikv03/rmpscraper/blob/main/rmpscraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import argparse\n",
        "import json\n",
        "import time\n",
        "\n",
        "# Constants for the Rate My Professor URL\n",
        "BASE_URL = \"https://www.ratemyprofessors.com/search/teachers\"\n",
        "\n",
        "# Function to get professor data from a specific page\n",
        "def get_professors_data(university, department, page=1):\n",
        "    params = {\n",
        "        \"query\": university,\n",
        "        \"sid\": department,\n",
        "        \"page\": page\n",
        "    }\n",
        "    response = requests.get(BASE_URL, params=params)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(f\"Failed to fetch data from Rate My Professors (status code {response.status_code})\")\n",
        "\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # This is a placeholder selector. You need to inspect the RMP site for actual structure.\n",
        "    professor_elements = soup.find_all('div', class_='professor')\n",
        "\n",
        "    professors = []\n",
        "\n",
        "    for prof in professor_elements:\n",
        "        try:\n",
        "            name = prof.find('h2').get_text(strip=True)\n",
        "            rating = prof.find('span', class_='rating').get_text(strip=True)\n",
        "            difficulty = prof.find('span', class_='difficulty').get_text(strip=True)\n",
        "            comments = prof.find('div', class_='comments').get_text(strip=True)\n",
        "            professors.append({\n",
        "                'name': name,\n",
        "                'rating': rating,\n",
        "                'difficulty': difficulty,\n",
        "                'comments': comments\n",
        "            })\n",
        "        except AttributeError:\n",
        "            # Skip any entries that don't have all required fields\n",
        "            continue\n",
        "\n",
        "    return professors\n",
        "\n",
        "# Function to handle pagination\n",
        "def scrape_all_professors(university, department):\n",
        "    all_professors = []\n",
        "    page = 1\n",
        "\n",
        "    while True:\n",
        "        print(f\"Scraping page {page}...\")\n",
        "        try:\n",
        "            professors = get_professors_data(university, department, page)\n",
        "            if not professors:\n",
        "                break\n",
        "            all_professors.extend(professors)\n",
        "            page += 1\n",
        "            time.sleep(1)  # Sleep to avoid overwhelming the server\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            break\n",
        "\n",
        "    return all_professors\n",
        "\n",
        "# Function to save data as CSV\n",
        "def save_as_csv(professors, output_file):\n",
        "    df = pd.DataFrame(professors)\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"Data saved to {output_file}\")\n",
        "\n",
        "# Function to save data as JSON\n",
        "def save_as_json(professors, output_file):\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(professors, f, indent=4)\n",
        "    print(f\"Data saved to {output_file}\")\n",
        "\n",
        "# Main function to handle command-line arguments and program flow\n",
        "def main():\n",
        "    # Removed argparse and added variables to allow function to run in Jupyter\n",
        "    university = \"Your University\" # Replace with your university\n",
        "    department = \"Your Department\" # Replace with your department\n",
        "    output = \"csv\" # Choose either csv or json\n",
        "\n",
        "    print(f\"Scraping professors from {university} in the {department} department...\")\n",
        "    professors = scrape_all_professors(university, department)\n",
        "\n",
        "    if output == 'csv':\n",
        "        save_as_csv(professors, f'{university}_professors.csv')\n",
        "    elif output == 'json':\n",
        "        save_as_json(professors, f'{university}_professors.json')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UjcKPlzbADI",
        "outputId": "6fb708a6-860b-4c65-f9e6-37e1feb57f6d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping professors from Your University in the Your Department department...\n",
            "Scraping page 1...\n",
            "Data saved to Your University_professors.csv\n"
          ]
        }
      ]
    }
  ]
}